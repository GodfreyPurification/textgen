{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a55efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (4.36.0.dev0)\n",
      "Requirement already satisfied: filelock in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\jupyterlab\\betterzilaimodel\\aimodelvenv\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06460cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproducability\n",
    "SEED = 34\n",
    "\n",
    "#maximum number of words in output text\n",
    "MAX_LEN = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3acaa452",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = \"I don't know about you, but there's only one thing I want to do after a long day of work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6517ca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\jupyterlab\\BetterzilAIModel\\aimodelvenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyterlab\\BetterzilAIModel\\aimodelvenv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLay  multiple                  774030080 \n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 774030080 (-1198846976.00 Byte)\n",
      "Trainable params: 774030080 (-1198846976.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\jupyterlab\\BetterzilAIModel\\aimodelvenv\\Lib\\site-packages\\keras\\src\\utils\\layer_utils.py:146: RuntimeWarning: overflow encountered in scalar add\n",
      "  total_memory_size += weight_shape * per_param_size\n"
     ]
    }
   ],
   "source": [
    "#get transformers\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#get large GPT2 tokenizer and GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#view model parameters\n",
    "GPT2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788100cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get deep learning basics\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81c3949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work: go to the gym.\n",
      "\n",
      "I'm not talking about the gym that's right next to my house. I'm talking about the gym that's right next to my office.\n",
      "\n",
      "I'm not talking about the gym that\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aee9702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
      "\n",
      "\"I know, I know,\" you say. \"But you're not going to like this one. It's not a good movie. I mean, it's\n",
      "1: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
      "\n",
      "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a girl\n",
      "2: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
      "\n",
      "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a woman\n",
      "3: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
      "\n",
      "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a beautiful\n",
      "4: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
      "\n",
      "\"I know, I know,\" you say. \"But you're not going to like this one. It's not a good movie. I'm not sure if\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = GPT2.generate(\n",
    "    input_ids, \n",
    "    max_length = MAX_LEN, \n",
    "    num_beams = 5, \n",
    "    no_repeat_ngram_size = 2, \n",
    "    num_return_sequences = 5, \n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "print('')\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "# now we have 3 output sequences\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f74724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work.\"\n",
      "\n",
      "\"Hmm. Must be quite the choice of words.\"\n",
      "\n",
      "\"Well, it's not a choice of words, but a need. I can't find the right answer until I find my answer.\"\n",
      "\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 0, \n",
    "                             temperature = 0.8\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "302f8521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work. I want to get out of here and go jogging. To go jogging.\"\n",
      "\n",
      "\"That may be true, but I don't really have much money to spare!\"\n",
      "\n",
      "\"That's true too. Why don ...\n"
     ]
    }
   ],
   "source": [
    "#sample from only top_k most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b5d0791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work: try out some dessert! Today I've got a total of four different fruit ice creams from The Baker's Dozen. I'm going to share three of them with you, each with a twist.\n",
      "\n",
      "One was made ...\n"
     ]
    }
   ],
   "source": [
    "#sample only from 80% most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_p = 0.8, \n",
    "                             top_k = 0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aafc06d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I don't know about you, but there's only one thing I want to do after a long day of work and this is one of it. I have to do something else. It's been quite an exciting couple of weeks at the office, haven't I?\n",
      "\n",
      "Makes you wonder about the people who didn't get the memo that a long day of work is about to turn into a long day of fun....\n",
      "\n",
      "1: I don't know about you, but there's only one thing I want to do after a long day of work, and that's take a nice, long shower.\"\n",
      "\n",
      "It turns out that you're not alone in this pursuit. The shower is a very important part of showering. You can't just have one big, unisex shower. If you take your shower in the middle of the day, the water may be cold, it may be hot, and it may be full of nasty chemicals.\n",
      "\n",
      "As a result of this confusion, you'll often find yourself taking showers for different purposes. If you're going to take a shower at night, you may do so in...\n",
      "\n",
      "2: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to go on an adventure! You know, to see if I can get myself to the bottom of the hole before some kind of horrible death and/or injury happens to me. The one thing I know for sure is that I have never, ever, played in a hole of my own creation. I can say that from experience. The hole is my work. I had no choice but to use it as a playground. I used to do some of my most interesting and imaginative work on the hole.\n",
      "\n",
      "When I say \"my hole,\" I mean the...\n",
      "\n",
      "3: I don't know about you, but there's only one thing I want to do after a long day of work… I want to make some popcorn. A good-sized bowl of popcorn.\" She turned to me, her expression unreadable. I looked over to where the two of them were sitting, and they looked at each other, before Anna made a sound of frustration, and then burst out into laughter. \"How is this possible, Elsa?\" Elsa turned away, but she couldn't help herself, and burst out laughing at Anna. \"It's so much better than a cold pizza!\" Anna cheered, and I just shrugged, and turned back to work.\n",
      "\n",
      "Elsa returned from the...\n",
      "\n",
      "4: I don't know about you, but there's only one thing I want to do after a long day of work. I want to relax and watch TV and not worry about work.\n",
      "\n",
      "My first stop on my weekend road trip is the nearest shopping mall.\n",
      "\n",
      "We are on the way to my aunt's house. She's a retired teacher. I can't wait to spend my weekends with her.\n",
      "\n",
      "After walking to the mall, I walked around the city. I looked around the different shopping centers and shops. There's so much to do in Japan.\n",
      "\n",
      "I picked out a few shops to visit in this shopping mall.\n",
      "\n",
      "One of them was a souvenir...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#combine both sampling techniques\n",
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .7,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85, \n",
    "                              num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a17b429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d928fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt1, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8bbfc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "According to National Geographic, scientists from the University of São Paulo, Brazil, discovered that the unicorns lived in a valley in the remote Andes mountains, near the village of Mato Grosso do Sul, on the Atlantic coast. The researchers found a number of large horned and bearded animals. They also found traces of humans living nearby, and the animals also carried traces of blood from humans.\n",
      "\n",
      "The team found the unicorn herd and its human visitors in a valley where they had been watching a herd...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c339f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = 'Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt2, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62eba665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
      "\n",
      "The former Disney star, 22, was spotted leaving the store on her bicycle with $500 in cash.\n",
      "\n",
      "The news came on a day that Cyrus had been spotted in Beverly Hills with rapper Drake.\n",
      "\n",
      "The two had met earlier in the day for a photo shoot, which resulted in them meeting the public.\n",
      "\n",
      "Scroll down for video\n",
      "\n",
      "The former Disney star was caught shoplifting from Abercrombie & Fitch on Hollywood Boulevard today\n",
      "\n",
      "Cyrus was spotted leaving the store on her bicycle with $500 in cash\n",
      "\n",
      "Cyrus was seen wearing a yellow top and white shorts with a blue and white striped...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85\n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d12bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = 'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt3, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2f9ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      "\n",
      "All of the orc warbands, including the ones from which they were drawn, began charging, leaving the Alliance's ranks behind. The two sides battled with the orc warriors in close combat, and even though the two sides continued to clash, the battle seemed to go on forever. Then, all at once, the entire horde of orcs turned, as if to charge the approaching orcs. The battle began again, and the battle continued. The battle raged on for a good ten minutes or so, until the orcs were surrounded and routed. The battle continued on for another ten minutes or so, until all of the orcs were dead. All of the...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc170804",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4 = \"For today’s homework assignment, please describe the reasons for the US Civil War.\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt4, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71826ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: For today’s homework assignment, please describe the reasons for the US Civil War.\n",
      "\n",
      "For more from The Week's Power Lunch, click here.\n",
      "\n",
      "Follow @dgbxny...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79c6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimodelvenv",
   "language": "python",
   "name": "aimodelvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
